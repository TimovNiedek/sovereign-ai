model_list:
  # CPU vLLM models, loaded in "local" profile
  - model_name: vllm-qwen3
    litellm_params:
      model: hosted_vllm/Qwen/Qwen3-1.7B  # add hosted_vllm/ prefix to route as OpenAI provider
      api_base: http://vllm-qwen3:8000/v1      # add api base for OpenAI compatible provider
  - model_name: vllm-llama3
    litellm_params:
      model: hosted_vllm/meta-llama/Llama-3.2-3B-Instruct
      api_base: http://vllm-llama3:8001/v1
  # GPU vLLM models, loaded in "remote" profile
  - model_name: vllm-qwen3-coder-30B-A3B-Instruct
    litellm_params:
      model: hosted_vllm/stelterlab/Qwen3-Coder-30B-A3B-Instruct-AWQ
      api_base: http://vllm-coder:8000/v1
  - model_name: vllm-gpt-oss-20b
    litellm_params:
      model: hosted_vllm/openai/gpt-oss-20b
      api_base: http://vllm-gpt-oss:8001/v1
litellm_settings:
  callbacks:
    - prometheus
  # https://docs.litellm.ai/docs/proxy/prometheus#initialize-budget-metrics-on-startup
  prometheus_initialize_budget_metrics: true
